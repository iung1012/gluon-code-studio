
interface GLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string | Array<{
    type: 'text' | 'image_url';
    text?: string;
    image_url?: {
      url: string;
    };
  }>;
}

interface GLMResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
}

interface GLMStreamResponse {
  choices: Array<{
    delta: {
      content?: string;
    };
    finish_reason?: string;
  }>;
}

interface StreamCallbacks {
  onProgress?: (content: string, isComplete: boolean) => void;
  onComplete?: (fullContent: string) => void;
  onError?: (error: Error) => void;
}

export class GLMApiService {
  private apiKey: string;
  private baseUrl = 'https://openrouter.ai/api/v1/chat/completions';
  private basicModel = 'moonshotai/kimi-k2:free';
  private proModel = 'z-ai/glm-4.5-air';
  private visionModel = 'z-ai/glm-4.5-air';

  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }

  getModel(): string {
    return this.basicModel;
  }

  private selectModel(hasImages: boolean, modelType: 'basic' | 'pro' = 'basic'): string {
    if (hasImages) {
      return this.visionModel;
    }
    return modelType === 'pro' ? this.proModel : this.basicModel;
  }

  async generateProjectStructure(prompt: string, images?: string[], callbacks?: StreamCallbacks, modelType: 'basic' | 'pro' = 'basic'): Promise<string> {
    const hasImages = images && images.length > 0;
    const selectedModel = this.selectModel(hasImages, modelType);
    
    console.log(`üéØ Using model: ${selectedModel} (images: ${hasImages}, mode: ${modelType})`);

    const content: Array<{type: 'text' | 'image_url'; text?: string; image_url?: {url: string}}> = [
      {
        type: 'text',
        text: `Crie um website completo HTML monol√≠tico para: ${prompt}`
      }
    ];

    // Adicionar imagens se fornecidas
    if (hasImages) {
      content.push({
        type: 'text',
        text: '\n\nImagens fornecidas pelo usu√°rio (integre-as no c√≥digo HTML):'
      });
      
      images!.forEach((image, index) => {
        content.push({
          type: 'image_url',
          image_url: {
            url: image
          }
        });
        content.push({
          type: 'text',
          text: `Imagem ${index + 1} (integre esta imagem no HTML usando a tag <img> com src como data URL)`
        });
      });
    }

    const messages: GLMMessage[] = [
      {
        role: 'system',
        content: `Voc√™ √© um desenvolvedor JavaScript especialista. Regras OBRIGAT√ìRIAS:

1. SEMPRE escreva c√≥digo JavaScript MONOLITO (arquivo HTML √∫nico)
2. Estrutura obrigat√≥ria:
   - HTML completo com DOCTYPE
   - CSS dentro de <style> no <head>
   - JavaScript dentro de <script> antes do </body>
3. Sempre forne√ßa c√≥digo completo funcional
4. Use apenas HTML, CSS e JavaScript vanilla
5. Se imagens forem fornecidas, integre-as diretamente no HTML usando as data URLs fornecidas

IMPORTANTE: Retorne APENAS o c√≥digo HTML completo, sem JSON, sem explica√ß√µes, sem formata√ß√£o adicional. Apenas o c√≥digo HTML puro que funciona diretamente no navegador.`
      },
      {
        role: 'user',
        content: content
      }
    ];

    return callbacks 
      ? this.callStreamingAPI(messages, callbacks, selectedModel) 
      : this.callAPI(messages, selectedModel);
  }

  async editSpecificPart(currentCode: string, editRequest: string, images?: string[], callbacks?: StreamCallbacks, modelType: 'basic' | 'pro' = 'basic'): Promise<string> {
    const hasImages = images && images.length > 0;
    const selectedModel = this.selectModel(hasImages, modelType);
    
    console.log(`üéØ Using model: ${selectedModel} (images: ${hasImages}, mode: ${modelType})`);

    const content: Array<{type: 'text' | 'image_url'; text?: string; image_url?: {url: string}}> = [
      {
        type: 'text',
        text: `C√ìDIGO HTML ATUAL COMPLETO:
${currentCode}

INSTRU√á√ÉO DE ALTERA√á√ÉO (execute imediatamente): ${editRequest}`
      }
    ];

    // Adicionar imagens se fornecidas
    if (hasImages) {
      content.push({
        type: 'text',
        text: '\n\nImagens fornecidas pelo usu√°rio (integre-as conforme solicitado):'
      });
      
      images!.forEach((image, index) => {
        content.push({
          type: 'image_url',
          image_url: {
            url: image
          }
        });
        content.push({
          type: 'text',
          text: `Imagem ${index + 1} (use esta imagem conforme a instru√ß√£o do usu√°rio)`
        });
      });
    }

    content.push({
      type: 'text',
      text: '\nRetorne o HTML completo modificado agora:'
    });

    const messages: GLMMessage[] = [
      {
        role: 'system',
        content: `Voc√™ √© um desenvolvedor JavaScript especialista. REGRAS CR√çTICAS:

1. Receba o c√≥digo HTML monol√≠tico atual completo
2. Identifique EXATAMENTE a parte que o usu√°rio quer alterar
3. Fa√ßa APENAS a altera√ß√£o solicitada
4. Mantenha TODO o resto do c√≥digo EXATAMENTE igual
5. SEMPRE retorne o arquivo HTML completo funcional
6. Se imagens forem fornecidas, integre-as conforme a instru√ß√£o do usu√°rio usando as data URLs

CR√çTICO: NUNCA responda com explica√ß√µes ou perguntas. SEMPRE retorne HTML completo v√°lido.
Se n√£o entender a solicita√ß√£o, fa√ßa uma interpreta√ß√£o inteligente e aplique a mudan√ßa.
NUNCA pergunte o que fazer - sempre execute a altera√ß√£o solicitada.

IMPORTANTE: Retorne APENAS o c√≥digo HTML completo, sem JSON, sem explica√ß√µes, sem formata√ß√£o adicional. Apenas o c√≥digo HTML puro que funciona diretamente no navegador.`
      },
      {
        role: 'user',
        content: content
      }
    ];

    console.log('üìù Sending edit request:', { 
      editRequest, 
      currentCodeLength: currentCode.length,
      imagesCount: images?.length || 0,
      selectedModel,
      modelType
    });

    return callbacks 
      ? this.callStreamingAPI(messages, callbacks, selectedModel) 
      : this.callAPI(messages, selectedModel);
  }

  private async callStreamingAPI(messages: GLMMessage[], callbacks: StreamCallbacks, model: string): Promise<string> {
    console.log('üöÄ Calling GLM Streaming API with model:', model);
    
    try {
      const requestBody = {
        model,
        messages,
        temperature: 0.4,
        max_tokens: 4000,
        top_p: 0.8,
        stream: true
      };

      console.log('üì§ Request body:', JSON.stringify(requestBody, null, 2));

      const response = await fetch(this.baseUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      console.log('üì° Response status:', response.status);
      console.log('üì° Response headers:', Object.fromEntries(response.headers.entries()));

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå GLM Streaming API error response:', response.status, errorText);
        
        let errorMessage = `GLM API error: ${response.status} ${response.statusText}`;
        try {
          const errorData = JSON.parse(errorText);
          if (errorData.error?.message) {
            errorMessage = `GLM API error: ${errorData.error.message}`;
          }
        } catch (e) {
          // Keep original error message if JSON parsing fails
        }
        
        const error = new Error(errorMessage);
        callbacks.onError?.(error);
        throw error;
      }

      const reader = response.body?.getReader();
      if (!reader) {
        const error = new Error('No readable stream in response');
        callbacks.onError?.(error);
        throw error;
      }

      let fullContent = '';
      const decoder = new TextDecoder();
      let buffer = '';

      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) {
            console.log('‚úÖ Streaming complete, total content length:', fullContent.length);
            callbacks.onProgress?.(fullContent, true);
            callbacks.onComplete?.(fullContent);
            break;
          }

          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop() || ''; // Keep incomplete line in buffer

          for (const line of lines) {
            if (line.trim() === '') continue;
            
            if (line.startsWith('data: ')) {
              const data = line.slice(6).trim();
              
              if (data === '[DONE]') {
                console.log('üèÅ Received [DONE] signal');
                continue;
              }

              try {
                const parsed: GLMStreamResponse = JSON.parse(data);
                const deltaContent = parsed.choices?.[0]?.delta?.content;
                
                if (deltaContent) {
                  fullContent += deltaContent;
                  callbacks.onProgress?.(fullContent, false);
                }

                if (parsed.choices?.[0]?.finish_reason) {
                  console.log('üèÅ Stream finished with reason:', parsed.choices[0].finish_reason);
                }
              } catch (parseError) {
                console.warn('‚ö†Ô∏è Failed to parse streaming chunk:', data.substring(0, 100));
              }
            }
          }
        }
      } finally {
        reader.releaseLock();
      }

      if (!fullContent || fullContent.trim().length === 0) {
        throw new Error('API retornou conte√∫do vazio');
      }

      return fullContent;
    } catch (error) {
      console.error('‚ùå Error calling GLM Streaming API:', error);
      callbacks.onError?.(error as Error);
      throw error;
    }
  }

  private async callAPI(messages: GLMMessage[], model: string): Promise<string> {
    console.log('üöÄ Calling GLM API with model:', model);
    
    try {
      const requestBody = {
        model,
        messages,
        temperature: 0.4,
        max_tokens: 4000,
        top_p: 0.8
      };

      console.log('üì§ Request body:', JSON.stringify(requestBody, null, 2));

      const response = await fetch(this.baseUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      console.log('üì° Response status:', response.status);

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå GLM API error response:', response.status, errorText);
        
        let errorMessage = `GLM API error: ${response.status} ${response.statusText}`;
        try {
          const errorData = JSON.parse(errorText);
          if (errorData.error?.message) {
            errorMessage = `GLM API error: ${errorData.error.message}`;
          }
        } catch (e) {
          // Keep original error message if JSON parsing fails
        }
        
        throw new Error(errorMessage);
      }

      const data: GLMResponse = await response.json();
      console.log('‚úÖ GLM API response received:', { 
        hasChoices: !!data.choices,
        choicesCount: data.choices?.length || 0
      });
      
      if (!data.choices || data.choices.length === 0) {
        throw new Error('No response from GLM API');
      }

      const content = data.choices[0].message.content;
      console.log('üìÑ Content preview:', content.substring(0, 200) + '...');
      
      if (!content || content.trim().length === 0) {
        throw new Error('API retornou conte√∫do vazio');
      }
      
      return content;
    } catch (error) {
      console.error('‚ùå Error calling GLM API:', error);
      throw error;
    }
  }
}
