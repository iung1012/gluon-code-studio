
interface GLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface GLMResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
}

interface GLMStreamResponse {
  choices: Array<{
    delta: {
      content?: string;
    };
    finish_reason?: string;
  }>;
}

interface StreamCallbacks {
  onProgress?: (content: string, isComplete: boolean) => void;
  onComplete?: (fullContent: string) => void;
  onError?: (error: Error) => void;
}

export class GLMApiService {
  private apiKey: string;
  private baseUrl = 'https://open.bigmodel.cn/api/paas/v4/chat/completions';
  private model = 'glm-4-flash';

  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }

  getModel(): string {
    return this.model;
  }

  async generateProjectStructure(prompt: string, callbacks?: StreamCallbacks): Promise<string> {
    const messages: GLMMessage[] = [
      {
        role: 'system',
        content: `Voc√™ √© um desenvolvedor JavaScript especialista. Regras OBRIGAT√ìRIAS:

1. SEMPRE escreva c√≥digo JavaScript MONOLITO (arquivo HTML √∫nico)
2. Estrutura obrigat√≥ria:
   - HTML completo com DOCTYPE
   - CSS dentro de <style> no <head>
   - JavaScript dentro de <script> antes do </body>
3. Sempre forne√ßa c√≥digo completo funcional
4. Use apenas HTML, CSS e JavaScript vanilla

IMPORTANTE: Retorne APENAS o c√≥digo HTML completo, sem JSON, sem explica√ß√µes, sem formata√ß√£o adicional. Apenas o c√≥digo HTML puro que funciona diretamente no navegador.`
      },
      {
        role: 'user',
        content: `Crie um website completo HTML monol√≠tico para: ${prompt}`
      }
    ];

    return callbacks 
      ? this.callStreamingAPI(messages, callbacks) 
      : this.callAPI(messages);
  }

  async editSpecificPart(currentCode: string, editRequest: string, callbacks?: StreamCallbacks): Promise<string> {
    const messages: GLMMessage[] = [
      {
        role: 'system',
        content: `Voc√™ √© um desenvolvedor JavaScript especialista. REGRAS CR√çTICAS:

1. Receba o c√≥digo HTML monol√≠tico atual completo
2. Identifique EXATAMENTE a parte que o usu√°rio quer alterar
3. Fa√ßa APENAS a altera√ß√£o solicitada
4. Mantenha TODO o resto do c√≥digo EXATAMENTE igual
5. SEMPRE retorne o arquivo HTML completo funcional

CR√çTICO: NUNCA responda com explica√ß√µes ou perguntas. SEMPRE retorne HTML completo v√°lido.
Se n√£o entender a solicita√ß√£o, fa√ßa uma interpreta√ß√£o inteligente e aplique a mudan√ßa.
NUNCA pergunte o que fazer - sempre execute a altera√ß√£o solicitada.

IMPORTANTE: Retorne APENAS o c√≥digo HTML completo, sem JSON, sem explica√ß√µes, sem formata√ß√£o adicional. Apenas o c√≥digo HTML puro que funciona diretamente no navegador.`
      },
      {
        role: 'user',
        content: `C√ìDIGO HTML ATUAL COMPLETO:
${currentCode}

INSTRU√á√ÉO DE ALTERA√á√ÉO (execute imediatamente): ${editRequest}

Retorne o HTML completo modificado agora:`
      }
    ];

    console.log('üìù Sending edit request:', { 
      editRequest, 
      currentCodeLength: currentCode.length
    });

    return callbacks 
      ? this.callStreamingAPI(messages, callbacks) 
      : this.callAPI(messages);
  }

  private async callStreamingAPI(messages: GLMMessage[], callbacks: StreamCallbacks): Promise<string> {
    console.log('üöÄ Calling GLM Streaming API with model:', this.model);
    
    try {
      const response = await fetch(this.baseUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.model,
          messages,
          temperature: 0.4,
          max_tokens: 8000,
          top_p: 0.7,
          stream: true
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå GLM Streaming API error response:', response.status, errorText);
        const error = new Error(`GLM API error: ${response.status} ${response.statusText}`);
        callbacks.onError?.(error);
        throw error;
      }

      const reader = response.body?.getReader();
      if (!reader) {
        const error = new Error('No readable stream in response');
        callbacks.onError?.(error);
        throw error;
      }

      let fullContent = '';
      const decoder = new TextDecoder();
      let buffer = '';

      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) {
            console.log('‚úÖ Streaming complete, total content length:', fullContent.length);
            callbacks.onProgress?.(fullContent, true);
            callbacks.onComplete?.(fullContent);
            break;
          }

          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop() || ''; // Keep incomplete line in buffer

          for (const line of lines) {
            if (line.trim() === '') continue;
            
            if (line.startsWith('data: ')) {
              const data = line.slice(6).trim();
              
              if (data === '[DONE]') {
                console.log('üèÅ Received [DONE] signal');
                continue;
              }

              try {
                const parsed: GLMStreamResponse = JSON.parse(data);
                const deltaContent = parsed.choices?.[0]?.delta?.content;
                
                if (deltaContent) {
                  fullContent += deltaContent;
                  callbacks.onProgress?.(fullContent, false);
                }

                if (parsed.choices?.[0]?.finish_reason) {
                  console.log('üèÅ Stream finished with reason:', parsed.choices[0].finish_reason);
                }
              } catch (parseError) {
                console.warn('‚ö†Ô∏è Failed to parse streaming chunk:', data.substring(0, 100));
              }
            }
          }
        }
      } finally {
        reader.releaseLock();
      }

      if (!fullContent || fullContent.trim().length === 0) {
        throw new Error('API retornou conte√∫do vazio');
      }

      return fullContent;
    } catch (error) {
      console.error('‚ùå Error calling GLM Streaming API:', error);
      callbacks.onError?.(error as Error);
      throw error;
    }
  }

  private async callAPI(messages: GLMMessage[]): Promise<string> {
    console.log('üöÄ Calling GLM API with model:', this.model);
    
    try {
      const response = await fetch(this.baseUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.model,
          messages,
          temperature: 0.4,
          max_tokens: 8000,
          top_p: 0.7
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå GLM API error response:', response.status, errorText);
        throw new Error(`GLM API error: ${response.status} ${response.statusText}`);
      }

      const data: GLMResponse = await response.json();
      console.log('‚úÖ GLM API response received:', { 
        hasChoices: !!data.choices,
        choicesCount: data.choices?.length || 0
      });
      
      if (!data.choices || data.choices.length === 0) {
        throw new Error('No response from GLM API');
      }

      const content = data.choices[0].message.content;
      console.log('üìÑ Content preview:', content.substring(0, 200) + '...');
      
      if (!content || content.trim().length === 0) {
        throw new Error('API retornou conte√∫do vazio');
      }
      
      return content;
    } catch (error) {
      console.error('‚ùå Error calling GLM API:', error);
      throw error;
    }
  }
}
